{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and hosting SageMaker Models using the Apache MXNet Module API\n",
    "\n",
    "The **SageMaker Python SDK** makes it easy to train and deploy MXNet models. In this example, we train a simple neural network using the Apache MXNet [Module API](https://mxnet.apache.org/api/python/module/module.html) and the driving score dataset.\n",
    "\n",
    "### Setup\n",
    "\n",
    "First we need to define a few variables that will be needed later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.38.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/8f/c8e7b13cfd4c911aa5ed1974d9b3ba2c14567cd756693b88d18a70eee4f9/sagemaker-1.38.6.tar.gz (219kB)\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 20.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting boto3>=1.9.213 (from sagemaker==1.38.6)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/a0/1d1993d62dc06da6e57616a87633408ed82d960ec6902c69afdaf10f840e/boto3-1.9.225-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 27.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (1.14.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (3.5.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (1.2.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (1.23)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (0.1.5)\n",
      "Requirement already satisfied: docker-compose>=1.23.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (1.24.0)\n",
      "Requirement already satisfied: requests<2.21,>=2.20.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (2.20.0)\n",
      "Collecting fabric>=2.0 (from sagemaker==1.38.6)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/47feeb00dae857f0fbd1153a61e902e54ed77ccdc578b371a514a3959a19/fabric-2.5.0-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 26.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: enum34>=1.1.6 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from sagemaker==1.38.6) (1.1.6)\n",
      "Collecting botocore<1.13.0,>=1.12.225 (from boto3>=1.9.213->sagemaker==1.38.6)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/28/8f0051b77f5586ab271a176cf50692dc338cf6aa922ed2dbf878deabf24e/botocore-1.12.225-py2.py3-none-any.whl (5.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.7MB 4.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from boto3>=1.9.213->sagemaker==1.38.6) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from boto3>=1.9.213->sagemaker==1.38.6) (0.2.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from protobuf>=3.1->sagemaker==1.38.6) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from protobuf>=3.1->sagemaker==1.38.6) (39.1.0)\n",
      "Requirement already satisfied: docker[ssh]<4.0,>=3.7.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (3.7.2)\n",
      "Requirement already satisfied: backports.ssl-match-hostname>=3.5; python_version < \"3.5\" in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (3.5.0.1)\n",
      "Requirement already satisfied: PyYAML<4.3,>=3.10 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (3.12)\n",
      "Requirement already satisfied: texttable<0.10,>=0.9.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (0.9.1)\n",
      "Requirement already satisfied: dockerpty<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (0.4.1)\n",
      "Requirement already satisfied: ipaddress>=1.0.16; python_version < \"3.3\" in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (1.0.22)\n",
      "Requirement already satisfied: websocket-client<1.0,>=0.32.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (0.56.0)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (0.6.2)\n",
      "Requirement already satisfied: jsonschema<3,>=2.5.1 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (2.6.0)\n",
      "Requirement already satisfied: cached-property<2,>=1.2.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker-compose>=1.23.0->sagemaker==1.38.6) (1.5.1)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from requests<2.21,>=2.20.0->sagemaker==1.38.6) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from requests<2.21,>=2.20.0->sagemaker==1.38.6) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from requests<2.21,>=2.20.0->sagemaker==1.38.6) (2019.3.9)\n",
      "Requirement already satisfied: paramiko>=2.4 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from fabric>=2.0->sagemaker==1.38.6) (2.4.2)\n",
      "Collecting invoke<2.0,>=1.3 (from fabric>=2.0->sagemaker==1.38.6)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/ba/ef37aa6f629f7d09565fd7bfcf3ad429af248947f107e765ccd15e4f1c43/invoke-1.3.0-py2-none-any.whl (205kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 39.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from botocore<1.13.0,>=1.12.225->boto3>=1.9.213->sagemaker==1.38.6) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from botocore<1.13.0,>=1.12.225->boto3>=1.9.213->sagemaker==1.38.6) (2.7.3)\n",
      "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from s3transfer<0.3.0,>=0.2.0->boto3>=1.9.213->sagemaker==1.38.6) (3.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from docker[ssh]<4.0,>=3.7.0->docker-compose>=1.23.0->sagemaker==1.38.6) (0.4.0)\n",
      "Requirement already satisfied: functools32 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from jsonschema<3,>=2.5.1->docker-compose>=1.23.0->sagemaker==1.38.6) (3.2.3.post2)\n",
      "Requirement already satisfied: cryptography>=1.5 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from paramiko>=2.4->fabric>=2.0->sagemaker==1.38.6) (2.3.1)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from paramiko>=2.4->fabric>=2.0->sagemaker==1.38.6) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from paramiko>=2.4->fabric>=2.0->sagemaker==1.38.6) (0.4.5)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from paramiko>=2.4->fabric>=2.0->sagemaker==1.38.6) (3.1.6)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.7 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from cryptography>=1.5->paramiko>=2.4->fabric>=2.0->sagemaker==1.38.6) (1.11.5)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from cryptography>=1.5->paramiko>=2.4->fabric>=2.0->sagemaker==1.38.6) (0.24.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/mxnet_p27/lib/python2.7/site-packages (from cffi!=1.11.3,>=1.7->cryptography>=1.5->paramiko>=2.4->fabric>=2.0->sagemaker==1.38.6) (2.18)\n",
      "Building wheels for collected packages: sagemaker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running setup.py bdist_wheel for sagemaker ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/75/cc/82/52125309cf9f3667c544ed09bc875c94f865e91974e7de4110\n",
      "Successfully built sagemaker\n",
      "\u001b[31mawscli 1.16.198 has requirement botocore==1.12.188, but you'll have botocore 1.12.225 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore, boto3, invoke, fabric, sagemaker\n",
      "  Found existing installation: botocore 1.12.188\n",
      "    Uninstalling botocore-1.12.188:\n",
      "      Successfully uninstalled botocore-1.12.188\n",
      "  Found existing installation: boto3 1.9.188\n",
      "    Uninstalling boto3-1.9.188:\n",
      "      Successfully uninstalled boto3-1.9.188\n",
      "  Found existing installation: sagemaker 1.33.0\n",
      "    Uninstalling sagemaker-1.33.0:\n",
      "      Successfully uninstalled sagemaker-1.33.0\n",
      "Successfully installed boto3-1.9.225 botocore-1.12.225 fabric-2.5.0 invoke-1.3.0 sagemaker-1.38.6\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "alabaster==0.7.10\n",
      "anaconda-client==1.6.14\n",
      "anaconda-project==0.8.2\n",
      "asn1crypto==0.24.0\n",
      "astroid==1.6.3\n",
      "astropy==2.0.6\n",
      "attrs==18.1.0\n",
      "autovizwidget==0.12.7\n",
      "awscli==1.16.198\n",
      "Babel==2.5.3\n",
      "backports-abc==0.5\n",
      "backports.functools-lru-cache==1.5\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.ssl-match-hostname==3.5.0.1\n",
      "bcrypt==3.1.6\n",
      "beautifulsoup4==4.6.0\n",
      "bitarray==0.8.1\n",
      "bkcharts==0.2\n",
      "blaze==0.11.3\n",
      "bleach==2.1.3\n",
      "bokeh==1.0.4\n",
      "boto==2.48.0\n",
      "boto3==1.9.225\n",
      "botocore==1.12.225\n",
      "Bottleneck==1.2.1\n",
      "cached-property==1.5.1\n",
      "cdecimal==2.3\n",
      "certifi==2019.3.9\n",
      "cffi==1.11.5\n",
      "chardet==3.0.4\n",
      "click==6.7\n",
      "cloudpickle==0.5.3\n",
      "clyent==1.2.2\n",
      "colorama==0.3.9\n",
      "configparser==3.5.0\n",
      "contextlib2==0.5.5\n",
      "cryptography==2.3.1\n",
      "cycler==0.10.0\n",
      "Cython==0.28.2\n",
      "cytoolz==0.9.0.1\n",
      "dask==0.17.5\n",
      "datashape==0.5.4\n",
      "decorator==4.3.0\n",
      "defusedxml==0.6.0\n",
      "distributed==1.21.8\n",
      "docker==3.7.2\n",
      "docker-compose==1.24.0\n",
      "docker-pycreds==0.4.0\n",
      "dockerpty==0.4.1\n",
      "docopt==0.6.2\n",
      "docutils==0.14\n",
      "entrypoints==0.2.3\n",
      "enum-compat==0.0.2\n",
      "enum34==1.1.6\n",
      "environment-kernels==1.1.1\n",
      "et-xmlfile==1.0.1\n",
      "fabric==2.5.0\n",
      "fastcache==1.0.2\n",
      "filelock==3.0.4\n",
      "Flask==1.0.2\n",
      "Flask-Cors==3.0.4\n",
      "funcsigs==1.0.2\n",
      "functools32==3.2.3.post2\n",
      "future==0.17.1\n",
      "futures==3.2.0\n",
      "gevent==1.3.0\n",
      "glob2==0.6\n",
      "gmpy2==2.0.8\n",
      "graphviz==0.8.4\n",
      "greenlet==0.4.13\n",
      "grin==1.2.1\n",
      "h5py==2.8.0\n",
      "hdijupyterutils==0.12.7\n",
      "heapdict==1.0.0\n",
      "html5lib==1.0.1\n",
      "idna==2.6\n",
      "imageio==2.3.0\n",
      "imagesize==1.0.0\n",
      "invoke==1.3.0\n",
      "ipaddress==1.0.22\n",
      "ipykernel==4.8.2\n",
      "ipyparallel==6.2.2\n",
      "ipython==5.7.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.4.0\n",
      "isort==4.3.4\n",
      "itsdangerous==0.24\n",
      "jdcal==1.4\n",
      "jedi==0.12.0\n",
      "Jinja2==2.10\n",
      "jmespath==0.9.4\n",
      "jsonschema==2.6.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.3\n",
      "jupyter-console==5.2.0\n",
      "jupyter-core==4.4.0\n",
      "jupyterlab==0.32.1\n",
      "jupyterlab-launcher==0.10.5\n",
      "Keras-Applications==1.0.7\n",
      "keras-mxnet==2.2.4.1\n",
      "Keras-Preprocessing==1.0.9\n",
      "kiwisolver==1.0.1\n",
      "lazy-object-proxy==1.3.1\n",
      "linecache2==1.0.0\n",
      "llvmlite==0.23.1\n",
      "locket==0.2.0\n",
      "lxml==4.2.1\n",
      "MarkupSafe==1.0\n",
      "matplotlib==2.2.2\n",
      "mccabe==0.6.1\n",
      "mistune==0.8.3\n",
      "mkl-fft==1.0.0\n",
      "mkl-random==1.0.1\n",
      "mock==3.0.5\n",
      "model-archiver==1.0.1\n",
      "more-itertools==4.1.0\n",
      "mpmath==1.0.0\n",
      "msgpack==0.6.0\n",
      "msgpack-python==0.5.6\n",
      "multipledispatch==0.5.0\n",
      "mxboard==0.1.0\n",
      "mxnet-cu100mkl==1.4.0\n",
      "mxnet-mkl==1.4.0\n",
      "mxnet-model-server==1.0.1\n",
      "nb-conda==2.2.1\n",
      "nb-conda-kernels==2.2.1\n",
      "nbconvert==5.4.1\n",
      "nbformat==4.4.0\n",
      "networkx==2.1\n",
      "nltk==3.3\n",
      "nose==1.3.7\n",
      "notebook==5.5.0\n",
      "numba==0.38.0+0.g2a2b772fc.dirty\n",
      "numexpr==2.6.5\n",
      "numpy==1.14.5\n",
      "numpydoc==0.8.0\n",
      "odo==0.5.1\n",
      "olefile==0.45.1\n",
      "onnx==1.2.1\n",
      "openpyxl==2.5.3\n",
      "packaging==17.1\n",
      "pandas==0.24.2\n",
      "pandocfilters==1.4.2\n",
      "paramiko==2.4.2\n",
      "parso==0.2.0\n",
      "partd==0.3.8\n",
      "path.py==11.0.1\n",
      "pathlib2==2.3.2\n",
      "patsy==0.5.0\n",
      "pep8==1.7.1\n",
      "pexpect==4.5.0\n",
      "pickleshare==0.7.4\n",
      "Pillow==5.2.0\n",
      "pkginfo==1.4.2\n",
      "plotly==2.7.0\n",
      "pluggy==0.6.0\n",
      "ply==3.11\n",
      "prompt-toolkit==1.0.15\n",
      "protobuf==3.5.2\n",
      "protobuf3-to-dict==0.1.5\n",
      "psutil==5.4.5\n",
      "psycopg2==2.7.5\n",
      "ptyprocess==0.5.2\n",
      "py==1.5.3\n",
      "py4j==0.10.7\n",
      "pyasn1==0.4.5\n",
      "pycairo==1.15.4\n",
      "pycodestyle==2.4.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.18\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.19.0\n",
      "pyflakes==1.6.0\n",
      "pygal==2.4.0\n",
      "Pygments==2.2.0\n",
      "pykerberos==1.2.1\n",
      "pylint==1.8.4\n",
      "PyNaCl==1.3.0\n",
      "pyodbc==4.0.23\n",
      "pyOpenSSL==18.0.0\n",
      "pyparsing==2.2.0\n",
      "PySocks==1.6.8\n",
      "pyspark==2.3.2\n",
      "pytest==3.5.1\n",
      "python-dateutil==2.7.3\n",
      "pytz==2018.4\n",
      "PyWavelets==0.5.2\n",
      "PyYAML==3.12\n",
      "pyzmq==17.0.0\n",
      "QtAwesome==0.4.4\n",
      "qtconsole==4.3.1\n",
      "QtPy==1.4.1\n",
      "requests==2.20.0\n",
      "requests-kerberos==0.12.0\n",
      "rope==0.10.7\n",
      "rsa==3.4.2\n",
      "ruamel-yaml==0.15.35\n",
      "s3fs==0.1.5\n",
      "s3transfer==0.2.0\n",
      "sagemaker==1.38.6\n",
      "sagemaker-pyspark==1.2.4\n",
      "scandir==1.7\n",
      "scikit-image==0.13.1\n",
      "scikit-learn==0.20.3\n",
      "scipy==1.2.1\n",
      "seaborn==0.8.1\n",
      "Send2Trash==1.5.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.11.0\n",
      "snowballstemmer==1.2.1\n",
      "sortedcollections==0.6.1\n",
      "sortedcontainers==1.5.10\n",
      "sparkmagic==0.12.5\n",
      "Sphinx==1.7.4\n",
      "sphinxcontrib-websupport==1.0.1\n",
      "spyder==3.2.8\n",
      "SQLAlchemy==1.2.11\n",
      "statsmodels==0.9.0\n",
      "subprocess32==3.5.0\n",
      "sympy==1.1.1\n",
      "tables==3.4.3\n",
      "tblib==1.3.2\n",
      "terminado==0.8.1\n",
      "testpath==0.3.1\n",
      "texttable==0.9.1\n",
      "toolz==0.9.0\n",
      "tornado==5.0.2\n",
      "traceback2==1.4.0\n",
      "traitlets==4.3.2\n",
      "typing==3.6.4\n",
      "typing-extensions==3.7.2\n",
      "unicodecsv==0.14.1\n",
      "unittest2==1.1.0\n",
      "urllib3==1.23\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "websocket-client==0.56.0\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.4.2\n",
      "wrapt==1.10.11\n",
      "xlrd==1.1.0\n",
      "XlsxWriter==1.0.4\n",
      "xlwt==1.3.0\n",
      "zict==0.1.3\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.38.6\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Location to save your custom code in tar.gz format.\n",
    "custom_code_upload_location = 's3://{}/customcode/mxnet'.format(bucket)\n",
    "\n",
    "# Location where results of model training are saved.\n",
    "model_artifacts_location = 's3://{}/artifacts'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training script\n",
    "\n",
    "The ``maintenance.py`` script provides all the code we need for training and hosting a SageMaker model. The script also checkpoints the model at the end of every epoch and saves the model graph, params and optimizer state in the folder `/opt/ml/checkpoints`. If the folder path does not exist then it will skip checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-540323643652\n"
     ]
    }
   ],
   "source": [
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\r\n",
      "import gzip\r\n",
      "import json\r\n",
      "import logging\r\n",
      "import os\r\n",
      "import struct\r\n",
      "import boto3\r\n",
      "\r\n",
      "import mxnet as mx\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "import pickle\r\n",
      "# from sklearn.preprocessing import StandardScaler\r\n",
      "# from sklearn.utils import shuffle\r\n",
      "\r\n",
      "\r\n",
      "def load_pickle_file(path):\r\n",
      "    with open(path, 'rb') as f:\r\n",
      "        data = pickle.load(f)\r\n",
      "\r\n",
      "    return data\r\n",
      "\r\n",
      "def load_data():\r\n",
      "    data = load_pickle_file('/opt/ml/code/data.pickle')\r\n",
      "    label = load_pickle_file('/opt/ml/code/label.pickle')\r\n",
      "    \r\n",
      "    return split_data(data, label)\r\n",
      "\r\n",
      "def split_data(data, label):\r\n",
      "\r\n",
      "    \r\n",
      "    X, y = (data, label)\r\n",
      "    # split dataset\r\n",
      "    train_data = X[:80, :].astype('float32')\r\n",
      "\r\n",
      "    train_label = y[:80]\r\n",
      "    val_data = X[80 :].astype('float32')\r\n",
      "    val_label = y[80:]\r\n",
      "    return train_data, train_label, val_data, val_label\r\n",
      "\r\n",
      "def build_graph():\r\n",
      "    data = mx.sym.var('data')\r\n",
      "    fc1 = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=128)\r\n",
      "    act1 = mx.sym.Activation(data=fc1, name='act1', act_type=\"relu\")\r\n",
      "    fc2 = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden=64)\r\n",
      "    act2 = mx.sym.Activation(data=fc2, name='act2', act_type=\"relu\")\r\n",
      "    fc3 = mx.sym.FullyConnected(data=act2, name='fc3', num_hidden=5)\r\n",
      "    return mx.sym.SoftmaxOutput(data=fc3, name='softmax', multi_output=False)\r\n",
      "\r\n",
      "\r\n",
      "def get_training_context(num_gpus):\r\n",
      "    if num_gpus:\r\n",
      "        return [mx.gpu(i) for i in range(num_gpus)]\r\n",
      "    else:\r\n",
      "        return mx.cpu()\r\n",
      "\r\n",
      "def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,\r\n",
      "          num_gpus, num_cpus, hosts, current_host, **kwargs):\r\n",
      "    train_images, train_labels, val_images, val_labels  = load_data()\r\n",
      "    batch_size = 10\r\n",
      "    train_iter = mx.io.NDArrayIter(train_images, train_labels, batch_size, shuffle=True)\r\n",
      "    val_iter = mx.io.NDArrayIter(val_images, val_labels, batch_size)\r\n",
      "    logging.getLogger().setLevel(logging.DEBUG)\r\n",
      "    kvstore = 'local' if len(hosts) == 1 else 'dist_sync'\r\n",
      "    mlp_model = mx.mod.Module(\r\n",
      "        symbol=build_graph(),\r\n",
      "        context=get_training_context(num_gpus))\r\n",
      "    mlp_model.fit(train_iter,\r\n",
      "                  eval_data=val_iter,\r\n",
      "                  kvstore=kvstore,\r\n",
      "                  optimizer='sgd',\r\n",
      "                  optimizer_params={'learning_rate': float(hyperparameters.get(\"learning_rate\", 0.1))},\r\n",
      "                  eval_metric='acc',\r\n",
      "                  batch_end_callback=mx.callback.Speedometer(batch_size, 100),\r\n",
      "                  num_epoch=40)\r\n",
      "    return mlp_model\r\n",
      "\r\n",
      "# if __name__ == '__main__':\r\n",
      "#     num_gpus = int(os.environ['SM_NUM_GPUS'])\r\n",
      "    \r\n",
      "#     train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,\r\n",
      "#           num_gpus, num_cpus, hosts, current_host)\r\n"
     ]
    }
   ],
   "source": [
    "!cat train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker's MXNet estimator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker ```MXNet``` estimator allows us to run single machine or distributed training in SageMaker, using CPU or GPU-based instances.\n",
    "\n",
    "When we create the estimator, we pass in the filename of our training script, the name of our IAM execution role, and the S3 locations we defined in the setup section. We also provide a few other parameters. ``train_instance_count`` and ``train_instance_type`` determine the number and type of SageMaker instances that will be used for the training job. The ``hyperparameters`` parameter is a ``dict`` of values that will be passed to your training script -- you can see how to access these values in the ``train.py`` script above.\n",
    "\n",
    "For this example, we will choose one ``ml.m4.xlarge`` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sagemaker\"\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "drvscore_estimator = MXNet(entry_point='train.py',\n",
    "                           role=role,\n",
    "                           output_path=model_artifacts_location,\n",
    "                           code_location=custom_code_upload_location,\n",
    "                           train_instance_count=1,\n",
    "                           train_instance_type='ml.m4.xlarge',\n",
    "                           framework_version='1.2.1',\n",
    "                           py_version='py2',\n",
    "                           dependencies=['data/data.pickle','data/label.pickle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our MXNet object, we can fit it using data stored in S3. Below we run SageMaker training on two input channels: **train** and **test**.\n",
    "\n",
    "During training, SageMaker makes this data stored in S3 available in the local filesystem where the script is running. The ```train.py``` script simply loads the train and test data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 13.7 ms, total: 29.7 ms\n",
      "Wall time: 24.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-10 17:47:16 Starting - Starting the training job..................\n",
      "2019-09-10 17:50:14 Starting - Launching requested ML instances.........\n",
      "2019-09-10 17:51:19 Starting - Preparing the instances for training......\n",
      "2019-09-10 17:52:44 Downloading - Downloading input data\n",
      "2019-09-10 17:52:44 Training - Downloading the training image...\n",
      "2019-09-10 17:53:14 Uploading - Uploading generated training model\n",
      "\u001b[31m2019-09-10 17:53:03,290 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:03,290 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:03,297 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:04,188 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,429 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 0, 'channels': {}, '_ps_verbose': 0, 'resource_config': {u'hosts': [u'algo-1'], u'network_interface_name': u'eth0', u'current_host': u'algo-1'}, 'user_script_name': u'train.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'train.py', u'sagemaker_submit_directory': u's3://sagemaker-us-east-1-540323643652/customcode/mxnet/connectedcar-drvscore8/source/sourcedir.tar.gz', u'sagemaker_region': u'us-east-1', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'connectedcar-drvscore8', u'sagemaker_container_log_level': 20}, 'hosts': [u'algo-1'], 'job_name': 'connectedcar-drvscore8', '_ps_port': 8000, 'user_script_archive': u's3://sagemaker-us-east-1-540323643652/customcode/mxnet/connectedcar-drvscore8/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-east-1', '_scheduler_ip': '10.2.148.184', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 4, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-540323643652/customcode/mxnet/connectedcar-drvscore8/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,771 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,856 INFO - root - Epoch[0] Train-accuracy=0.175000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,856 INFO - root - Epoch[0] Time cost=0.049\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,865 INFO - root - Epoch[0] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,892 INFO - root - Epoch[1] Train-accuracy=0.187500\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,892 INFO - root - Epoch[1] Time cost=0.027\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,904 INFO - root - Epoch[1] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,949 INFO - root - Epoch[2] Train-accuracy=0.175000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,949 INFO - root - Epoch[2] Time cost=0.045\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,958 INFO - root - Epoch[2] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,975 INFO - root - Epoch[3] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,976 INFO - root - Epoch[3] Time cost=0.018\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,982 INFO - root - Epoch[3] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,995 INFO - root - Epoch[4] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:10,995 INFO - root - Epoch[4] Time cost=0.013\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,000 INFO - root - Epoch[4] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,011 INFO - root - Epoch[5] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,011 INFO - root - Epoch[5] Time cost=0.011\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,024 INFO - root - Epoch[5] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,038 INFO - root - Epoch[6] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,047 INFO - root - Epoch[6] Time cost=0.023\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,071 INFO - root - Epoch[6] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,081 INFO - root - Epoch[7] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,082 INFO - root - Epoch[7] Time cost=0.011\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,108 INFO - root - Epoch[7] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,138 INFO - root - Epoch[8] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,138 INFO - root - Epoch[8] Time cost=0.029\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,146 INFO - root - Epoch[8] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,164 INFO - root - Epoch[9] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,165 INFO - root - Epoch[9] Time cost=0.018\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,172 INFO - root - Epoch[9] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,194 INFO - root - Epoch[10] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,195 INFO - root - Epoch[10] Time cost=0.022\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,201 INFO - root - Epoch[10] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,228 INFO - root - Epoch[11] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,228 INFO - root - Epoch[11] Time cost=0.026\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,237 INFO - root - Epoch[11] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,263 INFO - root - Epoch[12] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,263 INFO - root - Epoch[12] Time cost=0.026\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,269 INFO - root - Epoch[12] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,283 INFO - root - Epoch[13] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,283 INFO - root - Epoch[13] Time cost=0.014\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,287 INFO - root - Epoch[13] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,302 INFO - root - Epoch[14] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,303 INFO - root - Epoch[14] Time cost=0.015\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,308 INFO - root - Epoch[14] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,325 INFO - root - Epoch[15] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,325 INFO - root - Epoch[15] Time cost=0.016\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,330 INFO - root - Epoch[15] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,346 INFO - root - Epoch[16] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,347 INFO - root - Epoch[16] Time cost=0.016\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,352 INFO - root - Epoch[16] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,369 INFO - root - Epoch[17] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,369 INFO - root - Epoch[17] Time cost=0.017\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,374 INFO - root - Epoch[17] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,391 INFO - root - Epoch[18] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,391 INFO - root - Epoch[18] Time cost=0.017\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,396 INFO - root - Epoch[18] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,412 INFO - root - Epoch[19] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,413 INFO - root - Epoch[19] Time cost=0.016\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,418 INFO - root - Epoch[19] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,435 INFO - root - Epoch[20] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,435 INFO - root - Epoch[20] Time cost=0.017\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,440 INFO - root - Epoch[20] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,456 INFO - root - Epoch[21] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,456 INFO - root - Epoch[21] Time cost=0.016\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,462 INFO - root - Epoch[21] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,482 INFO - root - Epoch[22] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,483 INFO - root - Epoch[22] Time cost=0.021\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,489 INFO - root - Epoch[22] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,511 INFO - root - Epoch[23] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,511 INFO - root - Epoch[23] Time cost=0.022\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,518 INFO - root - Epoch[23] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,539 INFO - root - Epoch[24] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,539 INFO - root - Epoch[24] Time cost=0.021\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,546 INFO - root - Epoch[24] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,565 INFO - root - Epoch[25] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,565 INFO - root - Epoch[25] Time cost=0.019\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,571 INFO - root - Epoch[25] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,593 INFO - root - Epoch[26] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,593 INFO - root - Epoch[26] Time cost=0.021\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,600 INFO - root - Epoch[26] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,620 INFO - root - Epoch[27] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,620 INFO - root - Epoch[27] Time cost=0.020\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,627 INFO - root - Epoch[27] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,647 INFO - root - Epoch[28] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,648 INFO - root - Epoch[28] Time cost=0.020\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,655 INFO - root - Epoch[28] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,671 INFO - root - Epoch[29] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,671 INFO - root - Epoch[29] Time cost=0.016\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,677 INFO - root - Epoch[29] Validation-accuracy=0.100000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2019-09-10 17:53:11,687 INFO - root - Epoch[30] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,688 INFO - root - Epoch[30] Time cost=0.010\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,693 INFO - root - Epoch[30] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,704 INFO - root - Epoch[31] Train-accuracy=0.225000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,704 INFO - root - Epoch[31] Time cost=0.011\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,710 INFO - root - Epoch[31] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,720 INFO - root - Epoch[32] Train-accuracy=0.250000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,720 INFO - root - Epoch[32] Time cost=0.010\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,724 INFO - root - Epoch[32] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,731 INFO - root - Epoch[33] Train-accuracy=0.250000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,731 INFO - root - Epoch[33] Time cost=0.007\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,735 INFO - root - Epoch[33] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,742 INFO - root - Epoch[34] Train-accuracy=0.237500\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,742 INFO - root - Epoch[34] Time cost=0.007\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,746 INFO - root - Epoch[34] Validation-accuracy=0.100000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,753 INFO - root - Epoch[35] Train-accuracy=0.200000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,753 INFO - root - Epoch[35] Time cost=0.007\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,756 INFO - root - Epoch[35] Validation-accuracy=0.200000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,769 INFO - root - Epoch[36] Train-accuracy=0.187500\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,769 INFO - root - Epoch[36] Time cost=0.013\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,774 INFO - root - Epoch[36] Validation-accuracy=0.200000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,783 INFO - root - Epoch[37] Train-accuracy=0.187500\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,783 INFO - root - Epoch[37] Time cost=0.009\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,788 INFO - root - Epoch[37] Validation-accuracy=0.200000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,795 INFO - root - Epoch[38] Train-accuracy=0.150000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,795 INFO - root - Epoch[38] Time cost=0.007\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,798 INFO - root - Epoch[38] Validation-accuracy=0.200000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,806 INFO - root - Epoch[39] Train-accuracy=0.125000\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,806 INFO - root - Epoch[39] Time cost=0.008\u001b[0m\n",
      "\u001b[31m2019-09-10 17:53:11,810 INFO - root - Epoch[39] Validation-accuracy=0.250000\u001b[0m\n",
      "\n",
      "2019-09-10 17:53:19 Completed - Training job completed\n",
      "Training seconds: 50\n",
      "Billable seconds: 50\n"
     ]
    }
   ],
   "source": [
    "drvscore_estimator.fit(job_name='connectedcar-drvscore8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Creating an inference Endpoint\n",
    "\n",
    "After training, we use the ``MXNet estimator`` object to build and deploy an ``MXNetPredictor``. This creates a Sagemaker **Endpoint** -- a hosted prediction service that we can use to perform inference. \n",
    "\n",
    "The arguments to the ``deploy`` function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances. Here we will deploy the model to a single ``ml.m4.xlarge`` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = drvscore_estimator.deploy(initial_instance_count=1,\n",
    "#                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Making an inference request\n",
    "\n",
    "Now that our Endpoint is deployed and we have a ``predictor`` object, we can use it to run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data you want to run inference\n",
    "#data = ...\n",
    "#response = predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Delete the Endpoint\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#print(\"Endpoint name: \" + predictor.endpoint)\n",
    "#predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
